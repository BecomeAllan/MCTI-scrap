{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MCTI-SCRAP.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOzRfJx5ZvAkffO/0VDnZd/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BecomeAllan/MCTI-scrap/blob/main/MCTI_SCRAP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scrap functions\n",
        "\n",
        "Main call para buscar informações dos dados"
      ],
      "metadata": {
        "id": "20EWhPSC2v3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from royal import *\n",
        "from thegef import *\n",
        "from iadb import *\n",
        "from ICGEB import *\n",
        "from speciesconservation import *\n",
        "from ramsar import *\n",
        "from globalwomennet import *\n",
        "from arcadia import *\n",
        "from ruffordj import *\n",
        "\n",
        "funs = [\n",
        "        # royal1,\n",
        "        # royal2,\n",
        "        # royal3,\n",
        "        # thegef2,\n",
        "        # thegef3,\n",
        "        # iadb4,\n",
        "        # icgeb1,\n",
        "        # icgeb2,\n",
        "        # icgeb3,\n",
        "        # speciesconservation1,\n",
        "        # speciesconservation2,\n",
        "        # speciesconservation3,\n",
        "        # speciesconservation4,\n",
        "        # ramsar2,\n",
        "        # ramsar3,\n",
        "        globalwomennet1,\n",
        "        # globalwomennet2,\n",
        "        globalwomennet3,\n",
        "        arcadia2,\n",
        "        arcadia3,\n",
        "        # arcadia4,\n",
        "        rufford1,\n",
        "        rufford2,\n",
        "        rufford3,\n",
        "        rufford4\n",
        "        ]\n",
        "\n",
        "\n",
        "def call(fun, *args, **keyargs):\n",
        "  return fun(*args, **keyargs)"
      ],
      "metadata": {
        "id": "TaZ_OQJfyKIZ"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PPFcentral"
      ],
      "metadata": {
        "id": "AxZ9B7Py5Pd3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_I2XEyh4voj",
        "outputId": "896a1cc3-0960-46e0-a8a7-b293506bb246"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Diretório já existente\n",
            "Diretório já existente\n",
            "[globalwomennet][oportunidades][start]\n",
            "[globalwomennet][noticias][start]\n",
            "[globalwomennet][noticias][end]\n",
            "[arcadia][arcadia2][start]\n",
            "[globalwomennet][oportunidades][start]\n",
            "[arcadia][arcadia3][start]\n",
            "[arcadia][arcadia3][end]\n",
            "[rufford][rufford1][start]\n",
            "[rufford][rufford1][end]\n",
            "[rufford][rufford2][start]\n",
            "[arcadia][arcadia2][end]\n",
            "[rufford][rufford3][start]\n",
            "[rufford][rufford3][end]\n",
            "[rufford][rufford4][start]\n",
            "[rufford][rufford2][end]\n",
            "Função encerrada\n",
            "[rufford][rufford4][end]\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 1\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 2\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 3\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 4\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 5\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 6\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 7\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 8\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 9\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 10\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 11\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 12\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 13\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 14\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 15\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 16\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 17\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 18\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 19\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 20\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 21\n",
            "Arquivo do dia anterior não encontrado\n",
            "Concluido arquivo 22\n"
          ]
        }
      ],
      "source": [
        "from datetime import datetime\n",
        "import filecmp \n",
        "from filecmp import dircmp\n",
        "import os\n",
        "from os import walk\n",
        "import pandas as pd\n",
        "import numpy as np \n",
        "import shutil\n",
        "from itertools import compress\n",
        "from multiprocessing import Pool, freeze_support\n",
        "from itertools import repeat\n",
        "from functools import partial\n",
        "# Definindo o diretório para salvar os arquivos\n",
        "# os.chdir(r'D:\\Users\\beatriz.simoes\\Desktop\\codigos\\mcti-sefip-ppfcd2020\\output')\n",
        "#-------------------------- Criação das pastas -------------------\n",
        "# PASTA DIARIA\n",
        "dia = datetime.today().strftime('%y%m%d') # yy/mm/dd\n",
        "if os.path.exists(dia):\n",
        " print('Diretório já existente')   #nada acontece\n",
        "else:\n",
        "    os.makedirs(dia) #cria o diretório \n",
        "    print('Diretório criado')\n",
        "# PASTA PRINCIPAL\n",
        "baseprincipal = './/baseprincipal'\n",
        "if os.path.exists(baseprincipal):\n",
        " print('Diretório já existente')   #nada acontece\n",
        "else:\n",
        "    os.makedirs(baseprincipal) #cria o diretório \n",
        "    print('Diretório criado')\n",
        "\n",
        "#---------------------------------------------------------------------------\n",
        "#-------------------- SCRAPING PARA PASTA DIÁRIA:  -------------------------\n",
        "#---------------------------------------------------------------------------\n",
        "#  (aqui ficarão as importações dos arquivos py para scraping diário)\n",
        "\n",
        "try:\n",
        "  with Pool(2) as pool:\n",
        "    pool.map(partial(call, path='.//' + dia), funs)\n",
        "except:\n",
        "    print('Erro na extração, verificar arquivo fonte')\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------------------\n",
        "#--------------------------------- Funções  -----------------------------\n",
        "#---------------------------------------------------------------------------\n",
        "#--------------- função que define o caminho da pasta------------------\n",
        "def paths(pasta,arquivo):\n",
        "    path = '''.\\\\'''+pasta+'''\\\\'''+arquivo\n",
        "    return(path)\n",
        "#---------------------------------------------------------------------\n",
        "\n",
        "# --------------------------- função que atualiza a base --------------\n",
        "def atualizador(baseprincipal,diamaisrecente):\n",
        "    diario = pd.read_csv(diamaisrecente)\n",
        "    main = pd.read_csv(baseprincipal)\n",
        "    # checando o que do 'diario' está no 'main'\n",
        "    a = diario['link'].isin(main['link'])   # usar o tag/id criado inves do link\n",
        "    b = [not bool for bool in a] # inverter para o TRUE ser a linha que não tem no main\n",
        "    novaslinhas = diario[b]\n",
        "    main = main.append(novaslinhas,ignore_index=True) #novo main\n",
        "    main.to_csv(baseprincipal,index=False,sep=\",\")\n",
        "    print('A base foi atualizada')\n",
        "# -------------------------------------------------------------------\n",
        "\n",
        "#---------------------------------------------------------------------------\n",
        "#--------------------------------- Comparação  -----------------------------\n",
        "#---------------------------------------------------------------------------\n",
        "\n",
        "#------------   pegando todos os diretórios da pasta output\n",
        "\n",
        "_,dirnames,_  = next(walk('.'))\n",
        "dirnames.sort(reverse=True) # Os obj 0 vai ser a base principal. o 1 vai ser o dia mais recente e o 2 o dia anterior.\n",
        "\n",
        "_,_,filenamesDia = next(walk('.//'+dia)) # Arquivos extraídos no dia.\n",
        "_,_,filenamesBase = next(walk('.//baseprincipal')) # Arquivos extraídos na base.\n",
        "\n",
        "# -------------------------------- Se o arquivo não tiver na base:\n",
        "# Verificando se os nomes que estão no Dia estão na Base.\n",
        "a = [i in filenamesBase for i in  filenamesDia]\n",
        "a = [not bool for bool in a] # inversão pro True ser o arquivo faltante\n",
        "arquivos = list(compress(filenamesDia, a)) # arquivo que está faltando na baseprincipal\n",
        "for f in arquivos:\n",
        "    shutil.copy('.//'+dia+'//'+f, './/baseprincipal')\n",
        "# ------------------------------------------------------\n",
        "\n",
        "\n",
        "#---------------------------------------------------------------------------\n",
        "#--------------------------------- Atualização  ----------------------------\n",
        "#---------------------------------------------------------------------------\n",
        "filenames = filenamesDia\n",
        "\n",
        "for i in range(0,len(filenames)):\n",
        "    try:\n",
        "        base = str(paths(dirnames[0],filenames[i])) #base \n",
        "        dia1 = paths(dirnames[1],filenames[i]) #dia mais recente \n",
        "        dia2 = paths(dirnames[2],filenames[i]) #dia anterior \n",
        "        comp = filecmp.cmp(dia1, dia2, shallow = False)\n",
        "        if comp==False:\n",
        "            print('arquivos diários são diferentes, base atualizada')\n",
        "            atualizador(base,dia1)\n",
        "        else:\n",
        "            print('arquivos diários são iguais, base não atualizada')\n",
        "    except:\n",
        "      print(\"Arquivo do dia anterior não encontrado\")\n",
        "\n",
        "    print('Concluido arquivo '+str(i+1))\n",
        "\n",
        "# como a coluna codigo contem o dia, os arquivos sempre vão ser diferentes.\n"
      ]
    }
  ]
}